{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from time import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_features = 10000\n",
    "n_components = 10\n",
    "n_top_word = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading dataset...\")\n",
    "\n",
    "dataset = pd.read_csv('https://raw.githubusercontent.com/Matheusadler/Sentiment-Analysis-Application/master/g1_clean.csv', sep=',', encoding='UTF-8')\n",
    "\n",
    "data_samples = dataset.iloc[1:,0]\n",
    "\n",
    "n_samples = len(data_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded!\n"
     ]
    }
   ],
   "source": [
    "#Loading StopWords\n",
    "from io import open\n",
    "with open(\"preprocessamento\\essenciais\\stopwords_pt_nltk.txt\",\"r\",encoding='utf-8') as a:\n",
    "    StopWords = a.readlines()\n",
    "    StopWords = [w.replace('\\n', '') for w in StopWords]\n",
    "\n",
    "\n",
    "print(\"Dataset loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\ufeff',\n",
       " 'dia',\n",
       " 'dias',\n",
       " 'não',\n",
       " 'de',\n",
       " 'a',\n",
       " 'que',\n",
       " 'e',\n",
       " 'do',\n",
       " 'da',\n",
       " 'em',\n",
       " 'um',\n",
       " 'para',\n",
       " 'com',\n",
       " 'não',\n",
       " 'uma',\n",
       " 'os',\n",
       " 'no',\n",
       " 'se',\n",
       " 'na',\n",
       " 'por',\n",
       " 'mais',\n",
       " 'as',\n",
       " 'dos',\n",
       " 'como',\n",
       " 'mas',\n",
       " 'ao',\n",
       " 'ele',\n",
       " 'das',\n",
       " 'à',\n",
       " 'seu',\n",
       " 'sua',\n",
       " 'ou',\n",
       " 'quando',\n",
       " 'muito',\n",
       " 'nos',\n",
       " 'já',\n",
       " 'eu',\n",
       " 'também',\n",
       " 'só',\n",
       " 'pelo',\n",
       " 'pela',\n",
       " 'até',\n",
       " 'isso',\n",
       " 'ela',\n",
       " 'entre',\n",
       " 'depois',\n",
       " 'sem',\n",
       " 'mesmo',\n",
       " 'aos',\n",
       " 'seus',\n",
       " 'quem',\n",
       " 'nas',\n",
       " 'me',\n",
       " 'esse',\n",
       " 'eles',\n",
       " 'você',\n",
       " 'essa',\n",
       " 'num',\n",
       " 'nem',\n",
       " 'suas',\n",
       " 'meu',\n",
       " 'às',\n",
       " 'minha',\n",
       " 'numa',\n",
       " 'pelos',\n",
       " 'elas',\n",
       " 'qual',\n",
       " 'nós',\n",
       " 'lhe',\n",
       " 'deles',\n",
       " 'essas',\n",
       " 'esses',\n",
       " 'pelas',\n",
       " 'este',\n",
       " 'dele',\n",
       " 'tu',\n",
       " 'te',\n",
       " 'vocês',\n",
       " 'vos',\n",
       " 'lhes',\n",
       " 'meus',\n",
       " 'minhas',\n",
       " 'teu',\n",
       " 'tua',\n",
       " 'teus',\n",
       " 'tuas',\n",
       " 'nosso',\n",
       " 'nossa',\n",
       " 'nossos',\n",
       " 'nossas',\n",
       " 'dela',\n",
       " 'delas',\n",
       " 'esta',\n",
       " 'estes',\n",
       " 'estas',\n",
       " 'aquele',\n",
       " 'aquela',\n",
       " 'aqueles',\n",
       " 'aquelas',\n",
       " 'isto',\n",
       " 'aquilo',\n",
       " 'estou',\n",
       " 'está',\n",
       " 'estamos',\n",
       " 'estão',\n",
       " 'estive',\n",
       " 'esteve',\n",
       " 'estivemos',\n",
       " 'estiveram',\n",
       " 'estava',\n",
       " 'estávamos',\n",
       " 'estavam',\n",
       " 'estivera',\n",
       " 'estivéramos',\n",
       " 'esteja',\n",
       " 'estejamos',\n",
       " 'estejam',\n",
       " 'estivesse',\n",
       " 'estivéssemos',\n",
       " 'estivessem',\n",
       " 'estiver',\n",
       " 'estivermos',\n",
       " 'estiverem',\n",
       " 'hei',\n",
       " 'há',\n",
       " 'havemos',\n",
       " 'hão',\n",
       " 'houve',\n",
       " 'houvemos',\n",
       " 'houveram',\n",
       " 'houvera',\n",
       " 'houvéramos',\n",
       " 'haja',\n",
       " 'hajamos',\n",
       " 'hajam',\n",
       " 'houvesse',\n",
       " 'houvéssemos',\n",
       " 'houvessem',\n",
       " 'houver',\n",
       " 'houvermos',\n",
       " 'houverem',\n",
       " 'houverei',\n",
       " 'houverá',\n",
       " 'houveremos',\n",
       " 'houverão',\n",
       " 'houveria',\n",
       " 'houveríamos',\n",
       " 'houveriam',\n",
       " 'sou',\n",
       " 'somos',\n",
       " 'são',\n",
       " 'era',\n",
       " 'éramos',\n",
       " 'eram',\n",
       " 'fui',\n",
       " 'foi',\n",
       " 'fomos',\n",
       " 'foram',\n",
       " 'fora',\n",
       " 'fôramos',\n",
       " 'seja',\n",
       " 'sejamos',\n",
       " 'sejam',\n",
       " 'fosse',\n",
       " 'fôssemos',\n",
       " 'fossem',\n",
       " 'for',\n",
       " 'formos',\n",
       " 'forem',\n",
       " 'serei',\n",
       " 'será',\n",
       " 'seremos',\n",
       " 'serão',\n",
       " 'seria',\n",
       " 'seríamos',\n",
       " 'seriam',\n",
       " 'tenho',\n",
       " 'tem',\n",
       " 'temos',\n",
       " 'tém',\n",
       " 'tinha',\n",
       " 'tínhamos',\n",
       " 'tinham',\n",
       " 'tive',\n",
       " 'teve',\n",
       " 'tivemos',\n",
       " 'tiveram',\n",
       " 'tivera',\n",
       " 'tivéramos',\n",
       " 'tenha',\n",
       " 'tenhamos',\n",
       " 'tenham',\n",
       " 'tivesse',\n",
       " 'tivéssemos',\n",
       " 'tivessem',\n",
       " 'tiver',\n",
       " 'tivermos',\n",
       " 'tiverem',\n",
       " 'terei',\n",
       " 'terá',\n",
       " 'teremos',\n",
       " 'terão',\n",
       " 'teria',\n",
       " 'teríamos',\n",
       " 'teriam',\n",
       " 'sr',\n",
       " 'nr',\n",
       " 'pra',\n",
       " 'falou',\n",
       " 'ser',\n",
       " 'disse',\n",
       " 'problema',\n",
       " 'pois',\n",
       " 'dia',\n",
       " 'tudo',\n",
       " '2017',\n",
       " 'iria',\n",
       " 'nada',\n",
       " 'aqui',\n",
       " 'nao',\n",
       " 'ter',\n",
       " '10',\n",
       " '12',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " '11',\n",
       " '12',\n",
       " '13',\n",
       " '14',\n",
       " '15',\n",
       " '16',\n",
       " '17',\n",
       " '18',\n",
       " '19',\n",
       " '20',\n",
       " 'produto',\n",
       " 'aparelho',\n",
       " 'uso',\n",
       " 'falta de',\n",
       " 'que',\n",
       " 'em',\n",
       " 'Problema',\n",
       " 'veículo',\n",
       " 'VEICULO',\n",
       " 'PROBLEMA',\n",
       " 'Veículo',\n",
       " 'Veiculo',\n",
       " 'NA',\n",
       " 'Na',\n",
       " 'Comprei',\n",
       " 'problemas',\n",
       " 'funciona',\n",
       " 'defeito',\n",
       " 'DE',\n",
       " 'Problemas',\n",
       " 'PROBLEMAS',\n",
       " 'problema',\n",
       " 'RECLAME',\n",
       " 'AQUI',\n",
       " 'EDITADO',\n",
       " 'Editado',\n",
       " 'Reclame',\n",
       " 'PARA',\n",
       " 'DA',\n",
       " 'ANO',\n",
       " 'ano',\n",
       " 'Reclame Aqui',\n",
       " 'está',\n",
       " 'esta',\n",
       " 'Demora',\n",
       " 'NÃO',\n",
       " 'NAO',\n",
       " 'Nã',\n",
       " 'NÃ',\n",
       " 'está',\n",
       " 'a',\n",
       " 'fiat',\n",
       " 'apple',\n",
       " 'Fiat',\n",
       " 'Apple',\n",
       " ' Apple',\n",
       " 'da Fiat',\n",
       " ' Fiat',\n",
       " 'FIAT',\n",
       " 'Apple',\n",
       " '  Apple',\n",
       " 'APPLE',\n",
       " 'Dell',\n",
       " 'DELL',\n",
       " 'dell',\n",
       " 'Submarino',\n",
       " 'submarino',\n",
       " 'SUBMARINO',\n",
       " 'LG',\n",
       " 'lg',\n",
       " 'Lg',\n",
       " 'Quantum',\n",
       " 'QUANTUM',\n",
       " 'quantum',\n",
       " 'go',\n",
       " 'GO',\n",
       " 'br',\n",
       " 'www',\n",
       " 'BR',\n",
       " 'https',\n",
       " '.com',\n",
       " 'k10',\n",
       " '2018',\n",
       " '01',\n",
       " '02',\n",
       " '03',\n",
       " '04',\n",
       " '05',\n",
       " 'brasil',\n",
       " 'BRASIL',\n",
       " 'Brasil',\n",
       " 'reclameaqui',\n",
       " 'didia',\n",
       " 'karina',\n",
       " '25',\n",
       " 'Oi',\n",
       " 'OI',\n",
       " 'oi',\n",
       " 'Vivo',\n",
       " 'VIVO',\n",
       " 'vivo',\n",
       " 'NET',\n",
       " 'net',\n",
       " 'TIM',\n",
       " 'tim',\n",
       " 'Tim',\n",
       " 'Net',\n",
       " 'Claro',\n",
       " 'CLARO',\n",
       " 'claro',\n",
       " 'Sercomtel',\n",
       " 'SERCOMTEL',\n",
       " 'ALGAR',\n",
       " 'algar',\n",
       " 'Algar',\n",
       " 'editado Aqui',\n",
       " 'FS',\n",
       " 'VAS',\n",
       " 'VO',\n",
       " 'editado',\n",
       " 'Aqui']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PRESIDENTE LULA ÉPOCA AFIRMOU APÓS TRAGÉDIA OCORRIDA CLA QUE MÁXIMO ANOS SERIA LANÇADO VLS QUE ATÉ AGORA ISSO NUNCA FOI REALIDADE BASE LANÇAMENTO FOI RECONSTRUÍDA SOMENTE SEJA QUASE ANOS APÓS TRAGÉDIA OCORRIDA CENTRO',\n",
       " 'PRESIDENTE LULA ÉPOCA AFIRMOU APÓS TRAGÉDIA OCORRIDA CLA QUE MÁXIMO ANOS SERIA LANÇADO VLS QUE ATÉ AGORA ISSO NUNCA FOI REALIDADE BASE LANÇAMENTO FOI RECONSTRUÍDA SOMENTE SEJA QUASE ANOS APÓS TRAGÉDIA OCORRIDA CENTRO',\n",
       " 'Muito loko esse carro',\n",
       " 'Laércio Dantas não tração dianteira mas tração nas rodas fundo todas elas estão convencendo que esse sistema melhor para pilotos fim semana que andam pelas cidades trackdays por Nisso Audi está anos luz frente com Quattro Até Ferrari lançou Cansaram tomar pau dos mas largadas Tração traseira mata muita gente pois poucos sabem usá Ainda mais carro for realmente forte Esse caminho era inevitável',\n",
       " 'Será excelente carro pra quem está acostumado com asiáticos mas Mercedes com tração dianteira não engulo']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_comments = []\n",
    "for w in range(len(dataset.Comment)):\n",
    "  comment = dataset['Comment'].iloc[w]\n",
    "\n",
    "  # remove special characters and digits\n",
    "  comment  = re.sub(\"(\\\\d|\\\\W)+|\\w*\\d\\w*\",\" \",comment )\n",
    "  comment = ' '.join(s for s in comment.split() if (not any(c.isdigit() for c in s)) and len(s) > 2)\n",
    "  clean_comments.append(comment)\n",
    "\n",
    "clean_comments[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "tf-idf features extracted!\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words=StopWords)\n",
    "tfidf = tfidf_vectorizer.fit_transform(clean_comments)\n",
    "\n",
    "print(\"tf-idf features extracted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "tf features for LDA extraction is completed!\n"
     ]
    }
   ],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words=StopWords)\n",
    "tf = tf_vectorizer.fit_transform(clean_comments)\n",
    "\n",
    "print(\"tf features for LDA extraction is completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model (Frobenius norm) with tf-idf features, n_samples=186 and n_features=10000...\n",
      "done in 0.191s.\n",
      "\n",
      "Topics in NMF model (Frobenius norm):\n",
      "Topic #0: vai corolla povo carro coisa ainda gente\n",
      "Topic #1: bmw cla motor mercedes preço opinião demais\n",
      "Topic #2: mundo classe total fiasco estrelinha resultado mercedes\n",
      "Topic #3: tração audi dianteira melhor ainda traseira gente\n",
      "Topic #4: anda etc acho fusion boa azera conjunto\n",
      "Topic #5: cla chega versão meio mercedes alto vai\n",
      "Topic #6: anos ocorrida tragédia após cla lançado lançamento\n",
      "Topic #7: mil preço comprar melhor deve caro mercedes\n",
      "Topic #8: lucro querem imposto porte ganhar custo alem\n",
      "Topic #9: carro mercedes desse igual carros custa todo\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (Frobenius norm) with tf-idf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          alpha=.1, l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (Frobenius norm):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting the NMF model (generalized Kullback-Leibler divergence) with tf-idf features, n_samples=186 and n_features=10000...\n",
      "done in 0.207s.\n",
      "\n",
      "Topics in NMF model (generalized Kullback-Leibler divergence):\n",
      "Topic #0: vai carro custa povo pior governo coisa\n",
      "Topic #1: bmw caro preço cla motor mercedes todos\n",
      "Topic #2: mundo todo classe mercedes ver menos fiasco\n",
      "Topic #3: audi tração ainda dianteira gente muita melhor\n",
      "Topic #4: anda conjunto frente opinião mecânica ponto carros\n",
      "Topic #5: verdade cla falem difícil chega boa meio\n",
      "Topic #6: cla anos lançado nunca agora ocorrida lançamento\n",
      "Topic #7: mil comprar preço melhor preços vou uns\n",
      "Topic #8: civic querem lucro ganhar compra custo comprar\n",
      "Topic #9: carro mercedes desse carros etc pobre igual\n",
      "\n",
      "Fitting LDA models with tf features, n_samples=186 and n_features=10000...\n",
      "done in 0.255s.\n",
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: comprar dinheiro cadenza azera motor equipado ambos\n",
      "Topic #1: povo civic governo coisa difícil ver gustavo\n",
      "Topic #2: carro igual mercedes desse mim benz pro\n",
      "Topic #3: carro mil tração comprar deve preços devia\n",
      "Topic #4: corolla lucro carro vai grande comprar verdade\n",
      "Topic #5: pior preço marca caro pedido painel pensar\n",
      "Topic #6: mil mercedes carro vai gosto acho lindo\n",
      "Topic #7: bmw cla vai carros preço opinião pobre\n",
      "Topic #8: mil passado mercedes preço menos falem verdade\n",
      "Topic #9: cla querem anos chega versão pago meio\n",
      "\n",
      "Fiting LSA model\n",
      "\n",
      "Topics in LSA model:\n",
      "Topic #0: carro mercedes preço cla vai valor bmw\n",
      "Topic #1: cla bmw classe mercedes preço mundo motor\n",
      "Topic #2: mundo anda estrelinha fiasco resultado total vamos\n",
      "Topic #3: tração mundo total estrelinha resultado fiasco audi\n",
      "Topic #4: bmw anda frente cla conjunto etc mecânica\n",
      "Topic #5: chega meio versão rodas tração cla boa\n",
      "Topic #6: anos ocorrida após tragédia agora lançado lançamento\n",
      "Topic #7: mil preço melhor deve caro audi comprar\n",
      "Topic #8: lucro querem custo porte ganhar exorbitante desenvolvidos\n",
      "Topic #9: vai povo corolla coisa pior muita ainda\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fit the NMF model\n",
    "print(\"Fitting the NMF model (generalized Kullback-Leibler divergence) with \"\n",
    "      \"tf-idf features, n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "t0 = time()\n",
    "nmf = NMF(n_components=n_components, random_state=1,\n",
    "          beta_loss='kullback-leibler', solver='mu', max_iter=1000, alpha=.1,\n",
    "          l1_ratio=.5).fit(tfidf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in NMF model (generalized Kullback-Leibler divergence):\")\n",
    "tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n",
    "print_top_words(nmf, tfidf_feature_names, n_top_word)\n",
    "\n",
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "\n",
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_word)\n",
    "\n",
    "print(\"Fiting LSA model\")\n",
    "\n",
    "lsa = TruncatedSVD(n_components=n_components, n_iter=40, tol=0.01)\n",
    "\n",
    "lsa.fit(tf)\n",
    "\n",
    "print(\"\\nTopics in LSA model:\")\n",
    "\n",
    "print_top_words(lsa, tf_feature_names, n_top_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
